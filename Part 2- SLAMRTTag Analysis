1.	Process the single-end raw reads using the SlamDunk pipeline (https://t-neumann.github.io/slamdunk/) to perform T-to-C mutation-aware alignment and estimate the percentage of T-to-C mutations. 

 slamdunk all -r PATH/hg19.fa -b PATH/hg19_gtf.bed -o aligned -t 12 PATH/fastq_file.gz

2.	Sort the filtered BAM (binary alignment map) files produced by SlamDunk pipeline using SAMtools sort command:

 samtools sort -n -o output_BAM_File.bam input_BAM_File.bam

3.	Run the bam2bakR Snakemake pipeline (https://github.com/simonlabcode/bam2bakR) to generate a .cB file necessary for downstream analysis with BakR. Prepare the following input files: sorted BAM files from SlamDunk, a genome assembly file (FASTA), a gene transfer format (GTF) file, and a .yaml configuration file. Set the .yaml configuration file as the following, altering the sample info as necessary:

##########################################################################################################################################################################
# location of ALL (including -s4U control) bam files samples:

  WT_ctl_1: PATH/WT_ctl_1.bam
  KO_ctl_1: PATH/KO_ctl_1.bam
  WT_4SU_1: PATH/WT_4SU_1.bam
  KO_4SU_1: PATH/KO_4SU_1.bam
   
  WT_ctl_2: PATH/WT_ctl_2.bam
  KO_ctl_2: PATH/KO_ctl_2.bam
  WT_4SU_2: PATH/WT_4SU_2.bam
  KO_4SU_2: PATH/KO_4SU_2.bam

# sample IDs of -s4U control bam files (or leave blank if you have none)
# Needs to be same sample ID(s) that show up under samples entry
control_samples: ['WT_ctl_1', 'KO_ctl_1', 'WT_ctl_2', 'KO_ctl_2']


# location of annotation gtf file
annotation: PATH/hg19.gtf

# location of genome fasta file
genome_fasta: PATH/hg19.fa

######## Parameters you should probably double check ########

# Number of cpus to be used by pipeline
cpus: 4

# Format of reads
FORMAT: "SE" # (PE, SE, NU)
                    # [SE - single end reads]
                    # [NU - including non-unique] (not tested)

# Are you using the Windows subsystem for linux? 0 = Yes, 1 = No
WSL: 1

# Number of reads per fragment when splitting up bam file
fragment_size: 3500000

# Type of mutations of interest
mut_tracks: "TC" # ("TC", "GA", "TC,GA")

# whether to make .tdf files or not
make_tracks: True

# Minimum base quality to call mutation
minqual: 40

# Which columns to keep in final cB.csv.gz file
keepcols: "sample,sj,io,ei,ai,GF,XF,rname"

# String common to spike-in gene_ids in annotation gtf
  # If you have no spike-ins, then this should be "\"\"", i.e., a string containing ""
spikename: "\"\""

# If True, tracks will be normalized
normalize: True

######## Parameters you should NEVER alter ########
awkscript: workflow/scripts/fragment_sam.awk
mutcall: workflow/scripts/mut_call.py
mutcnt: workflow/scripts/count_triple.py
count2tracks: workflow/scripts/count_to_tracks.py

##########################################################################################################################################################################

4.	Run the bam2bakR Snakemake workflow with the following commands:
 conda activate complete_pipeline
 snakemake --cores 8

5.	Continue with downstream analysis using the BakR Bioconductor package (https://github.com/simonlabcode/bakR). BakR performs several critical analyses, including estimating RNA degradation rate constants and identifying changes in RNA stability across different experimental conditions.

# Load dependencies 
library(dplyr)
library(bakR)
library(purrr)

# Loading the cB data 
cB_data <- read.csv("/PATH/cB.csv")
cB_sub <- cB_data[,c(5, 1, 10, 2,11)]

# Setting up the bakRData objects for pulse conditions

cB_pulse<- cB_sub[(cB_sub$sample=="KO_4SU_1")|(cB_sub$sample=="KO_4SU_2")|(cB_sub$sample=="WT_4SU_1")|(cB_sub$sample=="WT_4SU_2")|
(cB_sub$sample=="KO_ctl_1")|(cB_sub$sample=="KO_ctl_2")|
(cB_sub$sample=="WT_ctl_1")|(cB_sub$sample=="WT_ctl_2"),]

tl<- vector(length=8)
tl<- c(4,4,0,0,4,4,0,0)
Exp_ID<- vector(length=8)
Exp_ID<- c(2,2,2,2,1,1,1,1)

metadf<- cbind(tl, Exp_ID)
rownames(metadf) <- c("KO_4SU_1", "KO_4SU_2", "KO_ctl_1", "KO_ctl_2",
                      "WT_4SU_1", "WT_4SU_2", "WT_ctl_1", "WT_ctl_2")

data_pulse<- bakRData(cB_pulse, metadf)

# Perform bakRFit using the Fast_Fit parameter first
fit_pulse <- bakRFit(data_pulse, NSS= FALSE, totcut=50)

# visualize differential kinetic analysis of transcripts using a volcano plot
plotVolcano(fit_pulse$Fast_Fit)

# Follow up with bakRFit using the more stringent Hybrid_Fit parameter
fit_pulse <- bakRFit(fit_pulse, HybridFit = TRUE, NSS = FALSE)

# visualize differential kinetic analysis of transcripts using a volcano plot
plotVolcano(fit_pulse$Hybrid_Fit)

# Retrieve differential kinetic analysis
dka_info <- fit_pulse[["Hybrid_Fit"]][["Effects_df"]]

# Retrieve global and localized degradation rate constants
kdeg_info <- fit_pulse[["Hybrid_Fit"]][["Kdeg_df"]]

# Divide into global (IgG) and localized (SC35) groups
kdeg_info_global <- kdeg_info[kdeg_info$Exp_ID==1,]
kdeg_info_localized <- kdeg_info[kdeg_info$Exp_ID==2,]

# Convert degradation rate constants into half-lives using the following formula
kdeg_info_global$half_lives <- 0.69314718056/kdeg_info_global$kdeg
kdeg_info_localized$half_lives <- 0.69314718056/kdeg_info_localized$kdeg
